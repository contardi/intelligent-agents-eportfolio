by Stoica, A

The rise of deep learning models may feel like an old story, simply because the pace of innovation has been so fast. Image generation has evolved from the amusing experiments of early tools like DALL-E 2, which produced strange but entertaining images to share with friends, into a flood of platforms and mobile applications capable of creating content so realistic that scrolling through social media now demands extra attention just to be able to identify whether or not what we see is real.

This increase in realism raises serious ethical concerns. Deepfakes and other generated media blur the boundaries of authenticity, threatening trust in online communication and traditional media content, since journalists are also having a hard time identifying fabricated content (Thomson et al., 2025). This uncertainty can have psychological consequences, especially for teenagers whose self-image is warped by online media exposure (Antonina, 2025). Older adults may also struggle to navigate increasingly deceptive online platforms, deepening their digital exclusion and confusion (Harris et al., 2022).

Another ethical dilemma lies in AI-generated music. In 2023 a song using cloned vocals of Drake and The Weeknd began circulating social media platforms and quickly went viral without anyone noticing its inauthenticity (The Guardian, 2023). This highlights questions of consent, authorship, and artistic integrity. The situation is even more challenging for smaller musicians, having to compete with algorithms capable of producing entire albums overnight.

In conclusion, while deep learning technologies have been showcasing remarkable creativity recently, they also demand urgent ethical attention and a legal framework that protects existing art and artists. Issues of authenticity, mental health, and artistic fairness must be addressed if society wishes to benefit from AI without forgetting the very idea of creativity.

### References

Antonina, U. (2025) ‘Teenage psychological well-being on AI-driven media platforms: The impact of algorithms on mental health’, Proceedings of the 2nd International Scientific and Practical Conference “Innovative Technologies for Training and Educating Young People”, Boston, USA, 14–17 January, pp. 264. International Science Group. Available at: https://www.isg.org/ (Accessed: 7 October 2025)

Harris, M. T., Blocker, K. A. and Rogers, W. A. (2022) ‘Older adults and smart technology: Facilitators and barriers to use’, Frontiers in Computer Science, 4, p. 835927. Available at: https://doi.org/10.3389/fcomp.2022.835927 (Accessed: 7 October 2025)

Thomson, T., Thomas, R., Riedlinger, M. and Matich, P. (2025) ‘Generative AI & journalism: Content, journalistic perceptions, and audience experiences’, RMIT University Report. Available at: https://doi.org/10.25439/rmt.28426049.v1 (Accessed: 7 October 2025)

The Guardian (2023) ‘AI song featuring fake Drake and Weeknd vocals pulled from streaming services’, The Guardian. Available at: https://www.theguardian.com/music/2023/apr/18/ai-song-featuring-fake-drake-and-weeknd-vocals-pulled-from-streaming-services (Accessed: 7 October 2025)




--------

## Peer Response

Hi Stoica,

You raise crucial concerns about the erosion of authenticity and artistic integrity in the age of deep learning. I agree that the realism of AI-generated media now challenges our ability to discern truth from fabrication, a phenomenon Verdoliva (2020) also describes as a “forensic arms race” between content creation and detection tools.

One possible measure to mitigate these issues is embedding provenance and authenticity frameworks within media platforms. The Coalition for Content Provenance and Authenticity (C2PA) has been developing open standards to cryptographically sign digital content at the point of creation, enabling audiences and journalists to verify whether an image or recording has been altered (C2PA, 2024). This could reduce misinformation while preserving legitimate creative uses of generative AI.

Regarding AI-generated music, licensing mechanisms could draw from existing copyright models used for sampling in the recording industry, where creators must disclose the data sources and seek consent from original artists (IFPI, 2023). Combined with watermarking technologies for audio synthesis models, these steps could safeguard both attribution and fair compensation.

Finally, education is an often overlooked safeguard. Media-literacy programmes, especially for young users, should teach critical awareness of AI-generated content. As Harris et al. (2022) note, different age groups experience distinct barriers when adapting to new technologies, so tailored training could strengthen resilience against deception.

Your post highlights why regulation alone is insufficient; transparency, consent, and literacy must evolve alongside technology.

### References

C2PA (2024) Technical specifications for content provenance and authenticity v1.3. Available at: https://c2pa.org/specifications/ (Accessed: 13 October 2025).

Harris, M. T., Blocker, K. A. and Rogers, W. A. (2022) ‘Older adults and smart technology: Facilitators and barriers to use’, Frontiers in Computer Science, 4, p. 835927. Available at: https://doi.org/10.3389/fcomp.2022.835927 (Accessed: 13 October 2025).

IFPI (2023) Global music report 2023. International Federation of the Phonographic Industry. Available at: https://www.ifpi.org/resources/ (Accessed: 13 October 2025).

Verdoliva, L. (2020) ‘Media forensics and deepfakes: An overview’, IEEE Journal of Selected Topics in Signal Processing, 14(5), pp. 910–932. doi: 10.1109/JSTSP.2020.3002101. Available at: https://ieeexplore.ieee.org/document/9115874 (Accessed: 13 October 2025).